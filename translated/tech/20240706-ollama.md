---
title: 什么是 Ollama？ 您需要了解的所有重要事项
date: <发布时间>
author:
  - fosscope-translation-team
  - excniesnied
  - <校对者ID>
banner: https://static.fosscope.com/articles_img/2024/07/ollama/everything-about-ollama.webp
cover: https://static.fosscope.com/articles_img/2024/07/ollama/everything-about-ollama.webp
categories:
  - 翻译
  - tech
tags:
  - AI
authorInfo: |
  via: https://itsfoss.com/ollama/

  作者：[Ankush Das](https://itsfoss.com/author/ankush/)
  选题：[白鼠Cysnies](https://github.com/cys2004)
  译者：[excniesnied](https://github.com/excniesnied)
  校对：[<校对者ID>](https://github.com/<校对者ID>)

  本文由 [FOSScope翻译组](https://github.com/FOSScope/TranslateProject) 原创编译，[开源观察](https://fosscope.com/) 荣誉推出
---

回答有关 Ollama 的常见问题。

<!-- more -->

Ollama 是一个免费开源项目，可让您在本地运行 [各种开源的<ruby>大语言模型<rt>LLM</rt></ruby>](https://itsfoss.com/open-source-llms/)。

无论您是想利用像 Codestral 这样的开源大型语言模型进行代码生成，还是想用 LlaMa 3 作为 [ChatGPT 的替代方案](https://itsfoss.com/open-source-chatgpt-alternatives/)，Ollama 都能实现这些功能。

但这些是您可以在本地系统中通过 Ollama 实现的目标。为了实现这些目标，您需要了解什么？您又该如何做到这一切呢？

在本文中，我将提供 Ollama 的所有基本信息。

## 什么是 Ollama？

[Ollama](https://ollama.com/) 是一款可以在本地运行大语言模型的免费开源工具。它支持 Linux（基于 Systemd 的<ruby>发行版<rt>distros</rt></ruby>）、Windows 和 macOS（<ruby>Apple 芯片<rt>Apple Silicon</rt></ruby>）。

Ollama 是一个<ruby>命令行界面<rt>CLI</rt></ruby>工具。您可以方便地使用 Ollama 下载大语言模型并在本地私密运行。只需几个命令，您就可以下载像 Llama 3、Mixtral 等模型。

可以这样理解，就像 Docker 一样，您可以通过 Docker 从中央仓库下载各种镜像，并在容器中运行它们。同样，您可以使用 Ollama 下载各种开源的大型语言模型，并终端中运行它们。

## 系统要求？

要想使用 Ollama，您需要一个能够运行 AI 模型的系统。

首先，您需要一块显卡来运行这些模型。如果只使用 CPU（或集成显卡），体验将会非常缓慢和痛苦。因此，不建议使用 CPU 或集成显卡。

其次，您需要确保拥有一块至少 5 TOPS 算力的 Nvidia 或 AMD 显卡。一些常见的支持的显卡型号包括：

- NVIDIA RTX 40x0 系列，NVIDIA RTX 30x0 系列，GTX 1650 Ti，GTX 750 Ti，GTX 750
- AMD Vega 64，AMD Radeon RX 6000 系列，AMD Radeon RX 7000 系列

您可以在 [Ollama 官方文档](https://github.com/ollama/ollama/blob/main/docs/gpu.md) 中找到支持的 GPU 的完整列表。

## Ollama 是否支持 TPU 或 NPU？

很遗憾，**目前 Ollama 官方不支持 TPU 或 NPU**。

如果您感兴趣的话，TPU（Tensor Processing Unit，张量处理单元）是 Google 专门为机器学习工作流定制的<ruby>集成电路<rt>IC</rt></ruby>。它可能不是运行机器学习模型最强大的解决方案。但在某些情况下，通过 Google Cloud 它能够提供更加方便、灵活和实惠的解决方案。

NPU 是神经处理单元（Neural Processing Unit），专门用于 AI 相关的任务。可以将其视为专门为 AI 设计的显卡。

有像 [ezrknpu](https://github.com/Pelochus/ezrknpu) 的项目试图提供一些可在 NPU 上运行的大语言模型。[Abhishek 在 ArmSoM Sige7 上进行过测试](https://itsfoss.com/arosom-sige7-review/)，但目前，Ollama 并不能利用 NPU，而是需要显卡。

## Ollama 可以仅使用 CPU 运行吗？

是的，可以，但应避免这样做。 Ollama 专为使用 Nvidia 或 AMD GPU 而设计。 它无法识别英特尔集成显卡。虽然您可以只在 CPU 上运行 Ollama，但即使您的 16 核处理器达到最大负荷，性能也会大打折扣。

## 如何安装 Ollama？

如果您想在 Linux 上开始使用 [Ollama](https://itsfoss.com/ollama-setup-linux/)，<ruby>我们<rt>It's FOSS</rt></ruby>有一个很棒的教程供您参考：

{% link https://itsfoss.com/ollama-setup-linux/ %}

## Ollama 模型

Ollama 致力于为您提供开放模型，其中一些允许商业使用，而另一些可能不允许。

这些模型由 Ollama 托管，您需要像这样使用 pull 命令下载：

```bash
ollama pull codestral
```

[![ollama model pull illustration](https://static.fosscope.com/articles_img/2024/07/ollama/ubuntu-ollama-model-pull.webp)](https://static.fosscope.com/articles_img/2024/07/ollama/ubuntu-ollama-model-pull.webp)

您的数据不会被用于训练大语言模型，因为它是在您的设备上本地运行的。然而，如果愿意，您可以将其部署为服务，与您的软件应用程序进行交互。您可以参考 [GitHub仓库](https://github.com/ollama/ollama) 以了解更多信息。

Ollama 提供的一些有用模型包括：

- **LLaMa 3:** 由 Meta  Inc. 开发的一系列开放模型，包括经过指令微调（instruction-tuned）和预训练（pre-trained）的变体。
- **Gemma:** 由 Google  DeepMind 开发并受到 Google  Gemini 启发的轻量级开放模型。
- **Mistral:**  一个功能强大的开源模型，遵循  Apache  2.0  许可证发布。
- **Codestral:** Mistral 的 AI 模型，在非生产许可证下，经  80 多种编程语言的训练。

您可以在 [Ollama  库](https://ollama.com/library?ref=itsfoss.com) 网站中找到所有可用的大语言模型。您可以查看详细信息并将其拉取到您的设备上使用。

如果您不需要某个模型，可以在 Linux 上使用以下命令简单地删除它：

```bash
ollama rm codestral
```

[![ollama removing model](https://static.fosscope.com/articles_img/2024/07/ollama/ollama-remove-model.webp)](https://static.fosscope.com/articles_img/2024/07/ollama/ollama-remove-model.webp)

## 模型存储在哪里？

有时用户报告称，即使在执行了删除命令之后，存储空间并未释放，这意味着删除操作没有成功。

因此，在这些情况下，或者如果您想通过<ruby>图形用户界面<rt>GUI</rt></ruby>或文件管理器删除多个模型，您需要知道存储位置。以下是下载模型的默认存储路径：

- Linux: `/usr/share/ollama/.ollama/models`
- Windows: `C:\Users\%username%\.ollama\models`
- macOS: `~/.ollama/models`

## 如何停止运行 Ollama？

对于 Windows/macOS 系统，您可以点击屏幕右下角或右上角（取决于您的任务栏位置）的系统托盘图标，然后点击“退出 Ollama”。

在 Linux 上，您需要输入以下命令来停止 Ollama 进程在后台运行：

```bash
sudo systemctl stop ollama
```

## 如何获取 Ollama 图形用户界面？

Ollama 是一个基于命令行界面的工具。因此，默认情况下，您无法通过图形用户界面与之交互或管理模型。然而，您可以安装 Web UI 工具或图形用户界面前端与 AI 模型交互，而无需使用命令行界面。您可以参考我们这里列出的选项来探索可能的选择：

{% link https://itsfoss.com/ollama-web-ui-tools/ %}

## 总结

尽管有许多类似的工具，Ollama 已成为最受欢迎的本地运行大语言模型的工具。安装不同大语言模型的便捷性使得它成为想要使用本地 AI 的初学者的理想选择。

我已经尝试回答了关于 Ollama 最常见的问题之一。如有其他问题，请随时在评论区提问。
